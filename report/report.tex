% IEEE standard conference template; to be used with:
%   spconf.sty  - LaTeX style file, and
%   IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------

\documentclass[letterpaper]{article}
\usepackage{spconf,amsmath,amssymb,graphicx,paralist,url}
\usepackage[utf8]{inputenc} 			% Zeichensatz
\usepackage[T1]{fontenc} 			% Umlaute unterstützen
\usepackage{float}
\usepackage[skip=3px]{caption}
\usepackage[boxed,vlined,linesnumbered]{algorithm2e}	% Pseudocode
\SetAlCapSkip{2ex}				
% \setalcapskip{2ex}							% caption unter code
\hyphenation{now-list}
\hyphenation{later-list}

% Example definitions.
% --------------------
% nice symbols for real and complex numbers
\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\C}[0]{\mathbb{C}}

% bold paragraph titles
\newcommand{\mypar}[1]{{\bf #1.}}

% Title.
% ------
\title{Parallel implementation of Fringe Search}
%
% Single address.
% ---------------
\name{Lukas Mosimann, Christian Zeman} 
\address{ETH Z\"urich\\Z\"urich, Switzerland}

% For example:
% ------------
%\address{School\\
%		 Department\\
%		 Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%		 {School A-B\\
%		 Department A-B\\
%		 Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%		 while at ...}}
%		 {School C-D\\
%		 Department C-D\\
%		 Address C-D}
%

\begin{document}
%\ninept
%
\maketitle
%

\begin{abstract}
We've implemented and parallelized Fringe Search, which isa single-pair shortest path algorithm, for a shared memory environment. By already using two cores, the parallel version outperformed the sequential one on our benchmark graphs without showing any noticeable compromises in terms of path quality.
\end{abstract}

\section{Introduction}\label{sec:intro}

This project was part of the course \textit{Design of Parallel and High-Performance Computing} given by Torsten Hoefler and Markus Püschel in autumn 2013 at ETH Zürich.


\mypar{Motivation} 
Pathfinding is an important problem occurring in many applications, especially in computer games and robotics. Fringe Search is a popular algorithm for single-pair shortest path problems. Unlike A* it doesn't guarantee to find the shortest path, but rather a path "short enough". The advantage of Fringe Search is that it generally outperforms A* as shown in \cite{fringe:05}.\\
In \cite{brand:09} a parallel implementation of Fringe Search has been developed for a distributed memory environment. The goal of this paper was the implementation of a fast parallel version of Fringe Search for a shared memory environment.

\mypar{Related work} This paper is mainly based on three papers. The Fringe Search algorithm has been introduced in \cite{fringe:05}. S. Brand and R. Bidarra have implemented a parallel version in a distributed memory environment, published in \cite{brand:09} and in \cite{brand:12}, whereas in this paper the implementation has been done for a shared memory environment.

\section{Background: Shortest Path Problem}\label{sec:background}
In this section we formally define the single-pair shortest path problem and we consider and introduce two algorithms that are used to solve it.

\mypar{Single-pair shortest path problem}
A problem where the goal is to find the shortest path between a given start and an end node in a directed or undirected graph. In our case we used a directed graph.

\mypar{A*}
A very popular algorithm that uses a best-first search approach for solving the single-pair shortest path problem. For the best-first search it uses a heuristic function, which estimates the distance to the end node. It always finds the shortest possible path as long as the heuristic function never overestimates the real distance. Therefore it's called optimal. A* uses a priority queue for selecting the best node and therefore each insertion into the queue has complexity $\mathcal{O}(\log n)$ with respect to the size of the queue.

\mypar{Fringe Search}
Another single-pair shortest path algorithm that is similar to A*, but instead of a priority queue that always has the most promising node first, it stores the nodes in a doubly linked list where an insertion has only complexity $\mathcal{O}(1)$ and visits a node if it's "promising enough" which is determined by a global threshold value that will be continuously increased. The path will not be optimal but it is generally faster than A*, as shown in \cite{fringe:05}. An example of the algorithm can be found in figure~\ref{fig:algo}.
\begin{figure}[h]\centering
  \includegraphics[scale=0.245]{fringe_rep.eps}
  \caption{Fringe Search example \label{fig:algo}}
\end{figure}


\section{Concept and implementation}\label{sec:impl}

In this section we will show how the implementation has been done, emphasize important aspects and illustrate the used concepts for locking. 

\subsection{Language and data structures}\label{ssec:lang}

All implementations have been done with C++11, OpenMP 3 and some inline assembly for the locks.

\mypar{Graph structure}
We use a directed graph implemented as adjacency list. This means that each node stores a list of pointers to its neighbours as well as the distance to these nodes. Basically, this distance can be everything as long as there exists a good heuristic cost function which does not overestimate the real distance. \\
In our experiments we interpreted the graph as nodes in a 2D plane for visualization purposes. Each node has its position stored, so we can estimate the distance between two nodes with the Euclidean distance (or the Manhattan distance for a rectangular grid).

\mypar{Status}
Each Node possesses a status. An \textit{inactive} node has not yet been visited. In the beginning, each node except the start node is inactive. A \textit{closed} node has an estimated cost that must be below the current threshold (i.e. the sum of the real cost from the start to this node and the estimated cost from this node to the end is lower than the current threshold). If a node is closed it won't be updated anymore, even if we later find out that there would be a shorter path to this node. A node with status \textit{now} is a node that possibly lies on the path between start and end. It has a closed neighbor, what means that we know the real distance from the start to this node. Also a node with status \textit{later} has a closed neighbor but the cost estimation of this node currently is above threshold, so the node won't be visited again until we increase the threshold or we find a better path.

\mypar{Linked lists}
We use two doubly linked lists we will henceforth call the \textit{nowlist} and the \textit{laterlist}. The nowlist generally contains the nodes that have status \textit{now} whereas the laterlist contains the nodes that have status \textit{later}. Whenever the nowlist becomes empty we increase the threshold and swap the two lists as we also swap the definition of the status (now will become later and vice versa). 

\subsection{From sequential to parallel}\label{ssec:seqpar}

As a first step we implemented a strictly sequential Fringe Search in order to have a basis for the parallel implementation and also as a reference for the benchmarks regarding the speedup of the parallel version. The pseudocode for Fringe Search can be found in algorithm \ref{algo:par}.

\mypar{Parallel Fringe Search}
Besides the necessary locks for the insertion and removal from the lists the threshold relaxation part (see lines \ref{code:thresh}-\ref{code:swap} in algorithm \ref{algo:par}) is the bottleneck because this part has to be done sequentially by only one thread and not before all threads reached this point.

\begin{algorithm}[t!]
\SetKwFunction{heuristicDist}{heuristicDist}
\SetKwFunction{reconstructPath}{reconstructPath}
\SetKw{oder}{ or }

\SetInd{0.5em}{0.5em}
\newcommand{\var}[1]{{\textit{#1}}}

add node \var{start} to \var{nowlist} \;
$\var{laterlist} \gets \emptyset$ \;
$\var{threshold} \gets \heuristicDist{\var{start}, \var{end}}$ \;
\While{$\var{nowlist} \neq \emptyset \oder \var{laterlist} \neq \emptyset $}{ 
	\While(\tcp*[f]{\textnormal{$\exists$ nodes $\leq$ \var{threshold}}}){$nowlist \neq \emptyset$}{
		\var{x} $\gets$ Node from \var{nowlist} \;
 		\eIf{$\var{x}.\var{distanceEstimation} \leq \var{threshold}$}{
 	 		\If{$\var{x}$ = $\var{end}$}{
				\Return $\reconstructPath{}$ \tcp*[f]{\textnormal{done}}
	 		}
 			$\var{x}.\var{status} \gets closed$ \;
			\ForEach{\textnormal{neighbour} $\var{nb}$}{
				\uIf{$\var{nb}.\var{status}$ = $ \var{now}$}{
					calculate new distance estimation \var{dist} \;
					\If{$\var{dist} < \var{nb}.\var{distanceEstimation}$}{
						$\var{nb}.\var{distanceEstimation} \gets \var{dist}$ \;
						$\var{nb}.\var{parent} \gets \var{x} $\;
						move $\var{nb}$ right behind $\var{x}$ in $\var{nowlist}$ \;
					}
				}
				\uElseIf{$\var{nb}.\var{status} = \var{later}$}{
					calculate new distance estimation \var{dist} \;
					\If{$\var{dist} < \var{nb}.\var{distanceEstimation}$}{
						$\var{nb}.\var{distanceEstimation} \gets \var{dist}$ \;
						$\var{nb}.\var{parent} \gets \var{x} $\;
						remove $\var{nb}$ from $\var{laterlist}$ \;					
						insert $\var{nb}$ right behind $\var{x}$ in $\var{nowlist}$ \nllabel{code:moveback}\;
						$\var{nb}.\var{status} \gets \var{now}$ \;
					}
				}
				\ElseIf{$\var{nb}.\var{status} = \var{inactive}$}{
					calculate $\var{nb}.\var{distanceEstimation}$ \;
					$\var{nb}.\var{parent} \gets \var{x}$\;
					insert $\var{nb}$ right behind $\var{x}$ in $\var{nowlist}$ \;
					$\var{nb}.\var{status} \gets \var{now} $ \;
				}
			}
			remove $\var{x}$ from $\var{nowlist}$ \;
		}{
			$\var{x}.\var{status} \gets \var{later}$\;
 			move $\var{x}$ into \var{laterlist} \nllabel{code:move}\;
 		}
	}
	increase $\var{threshold}$ \nllabel{code:thresh} \;
	swap $\var{nowlist} $ and $ laterlist$\nllabel{code:swaplist} \;
	swap the definition of $\var{now}$ and $\var{later}$ \nllabel{code:swap} \;
}
\Return -1 \tcp*[r]{\textnormal{no existing path to end}}
\caption{parallelizable Fringe Search with two lists\label{algo:par}}
\end{algorithm}

\mypar{Swapping the lists}
Whenever we relax the threshold, the nowlist and the laterlist and the states of the included nodes will get swapped (lines \ref{code:swaplist} and \ref{code:swap} in algorithm \ref{algo:par}). This means that the nowlist will become the laterlist and vice versa, as the nodes with status later are potentially below threshold after increasing it.\\
After the swapping we don't want to have all the threads starting from the first node in the nowlist but rather have them distributed over the whole nowlist. We achieve this by remembering the last node in the laterlist a thread has worked with (e.g. we save the last node moved into the laterlist at line \ref{code:move} in algorithm \ref{algo:par}). Like this, the probability is high that this node is in the nowlist after the swapping and therefore not all threads will start at the same node. Another advantage of this behavior is that we might profit from the cache effect because this node might still be in the cache.\\
But of course we have to check if this node truly is in the nowlist which can't be guaranteed (a node remembered at line \ref{code:move} could be moved back at line \ref{code:moveback}). If it's no longer there we just start from the first node in the new nowlist.

\subsection{Locking}\label{ssec:lock}

\mypar{Traversing the list}
Whenever a thread traverses the nowlist it tries to lock every node it encounters. It does not force a lock! It just tries and if it's successful it can work with this node (visit the adjacent nodes, etc.). If the thread fails trying to lock the node it means that another thread is working with exactly this node.\\
In this case we don't go just to the next node in the list as this might lead to some jam because the thread currently holding the lock will most likely try to lock the next node soon and like this the two threads would block each other quite often.\\
So instead of just going to the next node if we encounter a locked node, we skip a few nodes in order to achieve a nice distribution of the threads over the nowlist. Skipping 150 nodes has proven to provide a relatively good distribution which results in a faster runtime for large graphs (more than $10^6$ nodes).

\mypar{Lock type}
Next to the locking mechanism provided by OpenMP we've implemented the following locks
\begin{compactitem}
\item TAS: test-and-set lock
\item TAS EXP: TAS with exponential back-off
\end{compactitem}
by using inline assembly.

\mypar{Avoiding deadlocks}
%TODO: esch das do wörkli klar? Has afe chli gänderet, aber bemr noni ganz sechr :) v.a. de erst absatz esch no schwerig...
In order to avoid deadlocks we always lock in the same direction (see figure~\ref{fig:lock}). If we don't know whether a second node we would like to lock is in the correct direction (e.g. we want to lock a left neighbor of a node already locked in the nowlist) we must not force a lock. Not knowing  the direction can only occur if we want to move a node in the nowlist and this is only the case if we've found a shorter path to a node that already has a path that is below the previous threshold. In this case we can try locking it. If the try is successful we can update the path and move it, and if the try was not successful we just leave the old path that was already below the old threshold. This does not change the characteristic of the algorithm since the other node was also in the nowlist and so it could have been closed by another thread before anyway. \\
If we lock a node outside the two lists, this can be done without any problems, because no thread will ever try to lock more than one node outside the lists.\\
Acting like this, we prevent any possible circular wait dependencies and so no deadlocks can occur \cite{Coffman:71}.

\begin{figure}[h]\centering
  \includegraphics[scale=0.38]{locking.eps}
  \caption{Locking direction \label{fig:lock}}
\end{figure}

\mypar{Inserting nodes}
The sequence for inserting nodes into a list is shown in figure~\ref{fig:insert}.

\begin{figure}[h]\centering
  \includegraphics[scale=0.31]{insert.eps}
  \caption{Insert a node into the list \label{fig:insert}}
\end{figure}

\mypar{Removing nodes}
The sequence for physically removing nodes from a list is shown in  figure~\ref{fig:remove}. We implemented and tested two different strategies for removing the nodes. The first strategy is to physically remove the node immediately as shown in figure~\ref{fig:remove}. The other strategy is to just mark it as removed, go on and let other threads physically remove it while they are traversing the list (lazy deletion).

\mypar{Acquiring locks}
All locks are acquired by "optimistic locking". This means, that if we try to get locks on nodes with specific properties (e.g. the node must be predecessor of another node or the node must have a specific state), we spin/wait until we get the lock and after acquiring the lock we ensure the conditions are still true and if not we release the lock immediately.

\begin{figure}[h]\centering
  \includegraphics[scale=0.31]{remove.eps}
  \caption{Remove a node from the list. Note: The pointers of a removed node remain untouched so that if possible a traversing thread won't get lost. \label{fig:remove}}
\end{figure}



\section{Experimental Results}\label{sec:exp}

In this section we evaluate our implementation by looking at the speed of the sequential algorithm, the performance of the different locks, strong scaling, weak scaling and several properties of the algorithm depending the threshold relaxation parameter.

\subsection{Experimental setup}\label{ssec:setup}

\mypar{Hardware and compiler}
The experiments have been done on kanifushi.inf.ethz.ch, a computer with the following properties:
\begin{compactitem}
\item NUMA model with 32 CPUs on 4 nodes
\item 8 CPUs per node
\item Intel(R) Xeon(R) CPU E7- 4830 @ 2.13GHz
\item per CPU: 32KB L1 cache, 256KB L2 cache
\item per node: 24MB L3 cache, 16GB memory
\end{compactitem}
Because it's an implementation for a shared memory environment we've used only 1 node with 8 CPUs for the experiments. In order to do this the application had to be executed with the \textit{numactl} tool (e.g. \textit{numactl -{}-cpunodebind=0 -{}-membind=0 fringe}).\\
The code has been compiled with g++ v. 4.6.1 using O1 optimization. All of the following performance analysis plots are the result of 50 runs on 1 node.

\mypar{Graphs used for benchmarking}
Each experiment has been run on two different graph types with different obstacles, the "cross graph" and the "circle graph" which both are illustrated in figure~\ref{fig:graphs}. Both graphs have the following properties:
\begin{compactitem}
\item based on a regular grid with distance 1
\item 8 edges per node (except at the border)
\item each node is displaced randomly according to a normal distribution with $\sigma = 0.3$
\item start node is top left and end node is bottom right
\end{compactitem}

\begin{figure}[h]\centering
  \includegraphics[scale=0.3]{benchmark_graphs.eps}
  \caption{The graphs used for benchmarking: "cross graph" (left) and "circle graph" (right). The blue nodes represent the found path, the green nodes are "closed" and the red nodes have status now or later when the algorithm finished. \label{fig:graphs}}
\end{figure}

\subsection{Results}\label{ssec:results}

\mypar{Sequential Fringe Search}
In order to affirm good performance of the sequential version we tested the sequential Fringe Search against the A* search from the Boost Graph Library\footnote{\url{http://www.boost.org/doc/libs/1_55_0/libs/graph/doc/index.html}} as we couldn't find a reliable implementation of Fringe Search we could compare our code to.\\
Our implementation of Fringe Search proved to be much faster than A* from the Boost Graph Library (about 10 times as fast on a graph with $1024 \times 1024$ nodes with a threshold relaxation value of $1$). But of course one has to take into account that Fringe Search doesn't guarantee an optimal path like A*.

\mypar{Locks}
The locks we implemented with inline assembly (see section \ref{ssec:lock}) were significantly faster than the locks provided by OpenMP (see figure~\ref{fig:lock_bench}). Therefore we used the test-and-set lock with exponential back-off for our implementation and also the following benchmarks are based on the implementation with these locks.

\begin{figure}[h]\centering
  \includegraphics[scale=0.558]{lock_benchmark.eps}
  \caption{Strong scaling using different locks.\label{fig:lock_bench}}
\end{figure}


\mypar{Strong scaling}
One of the most important questions is how much faster the application gets by using more cores for a fixed problem size. The results are shown in figure~\ref{fig:strong_scaling}.
\begin{figure}[h]\centering
  \includegraphics[scale=0.558]{strong_scaling_speedup.eps}
  \caption{Strong scaling (speedup against sequential version) \label{fig:strong_scaling}}
\end{figure}
For both benchmark graphs the parallel version is faster than the sequential version if it uses 2 or more cores. Using the parallel version with one core is of course slower than the sequential version  because of the additional overhead and the locking.

\mypar{Weak scaling}
Because of the limitations in terms of speedup according to Amdahl's law that are also visible in figure~\ref{fig:strong_scaling}, the next important question is how the runtime behaves with an increasing problem size for an increasing number of cores. This means that we keep the problem size per core (number of nodes per core) constant.
\begin{figure}[h]\centering
  \includegraphics[scale=0.558]{weak_scaling.eps}
  \caption{Weak scaling\label{fig:weak_scaling}}
\end{figure}\\
As shown in figure~\ref{fig:weak_scaling} we still experience an increase in runtime with increasing problem size, but it's not that much and the behavior actually meets our expectations, because with increasing problem size we also increase the time the program spends in the sequential part (longer path will lead to more threshold updates). Because the shortest path in the Circle Graph is shorter than in the Cross Graph, we need less threshold updates and so the time does not increase that much.

\mypar{Normal versus lazy removal} Looking at the figures~\ref{fig:strong_scaling} and \ref{fig:weak_scaling} it's apparent that it doesn't matter which strategy for removing the nodes we use (lazy vs. normal - see section \ref{ssec:lock}). The difference in speed is negligible.Therefore we will only show the results with normal removal in the future plots.

\mypar{Path length versus $\#$ cores}
An interesting question is whether the path length gets worse the more threads we have. As mentioned in section \ref{ssec:lock} (avoiding deadlocks) we cannot always update a node with a better path if it's already below threshold and locked by another thread. And the more threads we have the higher is the possibility that this happens. This behavior is not ideal but it's still consistent with the definition of the algorithm.
\begin{figure}[h]\centering
  \includegraphics[scale=0.558]{error_cores.eps}
  \caption{Relative error in path length compared to A* depending on the number of cores\label{fig:error_cores}}
\end{figure}
As shown in figure~\ref{fig:error_cores} the number of cores/threads does not affect the quality of the path.

\mypar{Path length versus threshold relaxation}
The threshold relaxation value defines by how much we increase the threshold once we don't have any more nodes below threshold left. If we increase the threshold only by a small value we will just consider few new nodes but they all are relatively promising nodes and result in a short path. If we increase the threshold by a bigger value we'll have more nodes to look at, but not all of them will be that good in terms of path length.
\begin{figure}[h]\centering
  \includegraphics[scale=0.558]{error_threshold.eps}
  \caption{Relative error in path length compared to shortest path (A*) depending on threshold relaxation value\label{fig:error_thresh}}
\end{figure}
As you can see in figure~\ref{fig:error_thresh} the relative error gets quite high once we have a threshold relaxation value greater than 1 whereas the path quality is very good for small threshold relaxation values.

\mypar{Runtime versus threshold relaxation}
According to figure~\ref{fig:error_thresh}, a low threshold relaxation value leads to small errors in path length. However, by doing this we always just have a few nodes in our nowlist, so the different threads will have more collisions and we have to increase the threshold more often, which means that we will spend more time in the sequential part of the algorithm. Therefore we expect our algorithm to be slower the smaller the threshold relaxation value is, which is also the case, as it can be seen in figure~\ref{fig:runtime_thresh}.\\
\begin{figure}[h]\centering
  \includegraphics[scale=0.558]{runtime_threshold.eps}
  \caption{Runtime depending on threshold relaxation value\label{fig:runtime_thresh}}
\end{figure}
Another interesting aspect of figure \ref{fig:runtime_thresh} is, that the sequential version gets slower for very high relaxation values. This is due to the fact, that high thresholds at the start will lead to a relatively big fan-out (compare with figure~\ref{fig:graphs}) that one core alone won't be able to handle in sufficient time. By running the application in parallel there are more cores that can handle these relatively big nowlist and, as shown in figure~\ref{fig:speedup_thresh}, the turnaround is later with more cores.
\begin{figure}[h]\centering
  \includegraphics[scale=0.558]{speedup_threshold.eps}
  \caption{Speedup of parallel version versus sequential version using different threshold relaxation values\label{fig:speedup_thresh}}
\end{figure}


\section{Conclusions}

We implemented a sequential version of Fringe Search that was significantly faster than the A* implementation from the Boost Graph Library. Then we parallelized it for a shared memory environment. The important results are:
\begin{itemize}
\item The parallel version running on only two cores was already faster than the sequential version.
\item The parallel version didn't have any noticeable compromises in terms of path quality.
\item Strong scaling has shown to be quite good, but of course limited by having a sequential part in the algorithm.
\item Weak scaling is good but not perfect due to the increasing sequential part with increasing graph size.
\item The implementation can be tuned with the threshold relaxation parameter in order to meet the individual requirements concerning path quality and runtime.
\end{itemize}
There is still a significant sequential part which is increasing with the problem size (threshold relaxation and list swapping) which can't be avoided easily, but nevertheless we believe that this is a relatively fast implementation on a shared memory environment.\\
Next steps could be enhancing the application to make it also work in a distributed/hybrid memory environment.



% References should be produced using the bibtex program from suitable
% BiBTeX files (here: bibl_conf). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{bibl_conf}

\end{document}


