% IEEE standard conference template; to be used with:
%   spconf.sty  - LaTeX style file, and
%   IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------

\documentclass[letterpaper]{article}
\usepackage{spconf,amsmath,amssymb,graphicx,paralist,url}
\usepackage[utf8]{inputenc} 			% Zeichensatz
\usepackage[T1]{fontenc} 			% Umlaute unterstützen
\usepackage{float}
\usepackage[boxed,vlined,linesnumbered]{algorithm2e}	% Pseudocode
\SetAlCapSkip{2ex}				
%\setalcapskip{2ex}							% caption unter code
\hyphenation{now-list}
\hyphenation{later-list}

% Example definitions.
% --------------------
% nice symbols for real and complex numbers
\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\C}[0]{\mathbb{C}}

% bold paragraph titles
\newcommand{\mypar}[1]{{\bf #1.}}

% Title.
% ------
\title{Parallel implementation of Fringe Search}
%
% Single address.
% ---------------
\name{Lukas Mosimann, Christian Zeman} 
\address{ETH Z\"urich\\Z\"urich, Switzerland}

% For example:
% ------------
%\address{School\\
%		 Department\\
%		 Address}
%
% Two addresses (uncomment and modify for two-address case).
% ----------------------------------------------------------
%\twoauthors
%  {A. Author-one, B. Author-two\sthanks{Thanks to XYZ agency for funding.}}
%		 {School A-B\\
%		 Department A-B\\
%		 Address A-B}
%  {C. Author-three, D. Author-four\sthanks{The fourth author performed the work
%		 while at ...}}
%		 {School C-D\\
%		 Department C-D\\
%		 Address C-D}
%

\begin{document}
%\ninept
%
\maketitle
%

\begin{abstract}
Describe in concise words what you do, why you do it (not necessarily
in this order), and the main result.  The abstract has to be
self-contained and readable for a person in the general area. You
should write the abstract last.
\end{abstract}

\section{Introduction}\label{sec:intro}

This project was part of the course \textit{Design of Parallel and High-Performance Computing} given by Torsten Hoefler and Markus Püschel in autumn 2013 at ETH Zürich.


\mypar{Motivation} 
Pathfinding is an important problem occuring in many applications, especially in computer games and robotics. Fringe search is a popular algorithm for single-pair shortest path problems. Unlike A* it doesn't guarantee to find the shortest path, but rather a path "short enough". The advantage of Fringe Search is that it generally outperforms A* as shown in \cite{fringe:05}.\\
In \cite{brand:09} a parallel implementation of Fringe Search has been done for a distributed memory environment. The goal of this paper was the implementation and benchmarking of a parallel version of Fringe Search for a shared memory environment.

\mypar{Related work} This paper is mainly based on three papers. The Fringe Search algorithm was introduced in \cite{fringe:05}. S. Brand and R. Bidarra implemented a parallel version in a distributed memory environment described in \cite{brand:12} and a bit more specific in the Master Thesis of S. Brand \cite{brand:09}. In this paper the implementation was done for a shared memory environment.

\section{Background: Shortest Path Problem}\label{sec:background}
In this section we formally define the single-pair shortest path problem and we consider and introduce two algorithms that are used to solve it.

\mypar{Single-pair shortest path problem}
A problem where the goal is to find the shortest path between a given start and an end node in a directed or undirected graph. In our case we used a directed graph.

\mypar{A*}
A very popular algorithm that uses a best-first search approach for solving the single-pair shortest path problem. For the best-first search it uses a heuristic function (e.g. Euclidean distance or Manhattan distance to end node). It always finds the shortest possible path as long as the heuristic function never overestimates the real distance. Therefore it's called optimal. It uses a priority queue for the nodes and therefore each insertion into the queue has complexity $\mathcal{O}(\log n)$ with respect to the size of the queue.

\mypar{Fringe Search}
A single-pair shortest path algorithm that is similar to A*, but instead of a priority queue that always has the most promising node first, it stores the nodes in a doubly linked list where an insertion has the complexity $\mathcal{O}(1)$ and visits a node if it's "promising enough" which is determined by a global threshold value that will be continuously increased. It's not optimal (it doesn't guarantee to find the shortest path) but it is generally faster than A*, as shown in \cite{fringe:05}. An example of the algorithm can be found in figure \ref{fig:algo}.
\begin{figure}[h]\centering
  \includegraphics[scale=0.245]{fringe_rep.eps}
  \caption{Fringe Search example \label{fig:algo}}
\end{figure}


\section{Concept and implementation}\label{sec:impl}

In this section we will show how the implementation has been done, emphasise the important aspects of it and illustrate the used concepts for locking.

\subsection{Language and data structure}\label{ssec:lang}

\mypar{Language}
The implementation has been done with C++11 and OpenMP 3.1.

\mypar{Graph structure}
We used a directed graph implemented as adjacency list. This means that each node has a list of pointers to its neighbours as well as the distance to this node. Basically this distance can be everything as long as there exists a good heuristic cost function, which does not overestimate the real distance. \\
In our experiments we interpreted the graph as nodes in a 2D plane, because it is easy to visualize. Each node stores its position, so we can estimate the distance between to nodes with the Euclidian distance or the Manhattan Distance.

\mypar{Status}
Each Node stores a status. A node, which is \textit{inactive}, has not yet been visited. In the beginning, each node is inactive. A \textit{closed} Node has an estimated cost under the current threshold (i.e. the sum of the real cost from the start to this node and the estimated cost from this node to the end is lower than the current threshold). An \textit{now} Node is a Node, which probably is on the path between start and end. It has a closed neighbor, i.e. we know the real distance from the start to his node. Status \textit{later} means, that it also has e closed neighbor, but the cost estimation through this Node is over the current threshold, so the node will only be visited again, if we increase the threshold.

\mypar{Linked lists}
We used two doubly linked lists we will henceforth call the "nowlist" and the "laterlist". The nowlist generally contains the nodes that have status $now$ whereas the laterlist contains the nodes that have status $later$. Whenever the nowlist is empty we increase the threshold and swap the two lists as we also swap the definition of the status ($now$ will become $later$ and vice versa). 

\subsection{From sequential to parallel}\label{ssec:seqpar}

\mypar{Sequential Fringe Search}
As a first step we implemented a strictly sequential Fringe Search in order to have a basis for the parallel implementation and also as a reference for the benchmarks regarding the speedup of the parallel version. The pseudocode for the Fringe Search can be found in algorithm \ref{algo:par}.

\mypar{Parallel Fringe Search}
Besides the necessary locks for the insertion and removal from the lists the threshold relaxation part (see line \ref{code:thresh} in algorithm \ref{algo:par}) is the bottleneck because this part has to be done sequentially by only one thread, after all threads reached this point.

\begin{algorithm}[t!]
\SetKwFunction{heuristicDist}{heuristicDist}
\SetKwFunction{reconstructPath}{reconstructPath}

\SetInd{0.5em}{0.5em}
\newcommand{\var}[1]{{\textit{#1}}}

add node \var{start} to \var{nowlist} \;
$\var{laterlist} \gets \emptyset$ \;
$\var{threshold} \gets \heuristicDist{\var{start}, \var{end}}$ \;
\While{$\var{nowlist} \neq \emptyset \textnormal{ \&\& } \var{laterlist} \neq \emptyset $}{ 
	\While(\tcp*[f]{\textnormal{$\exists$ nodes $\leq$ threshold}}){$nowlist \neq \emptyset$}{
		\var{x} $\gets$ Node from \var{nowlist} \;
 		\eIf{$\var{x}.\var{distanceEstimation} \leq \var{threshold}$}{
 	 		\If{$\var{x}$ == $\var{end}$}{
				\Return $\reconstructPath{}$ \tcp*[f]{\textnormal{done}}
	 		}
 			$\var{x}.\var{status} \gets closed$ \;
			\ForEach{neighbour $\var{nb}$}{
				\uIf{$\var{nb}.\var{status}$ == $ now$}{
					calculate new distance estimation \var{dist} \;
					\If{$\var{dist} < \var{nb}.\var{distanceEstimation}$}{
						$\var{nb}.\var{distanceEstimation} = \var{dist}$ \;
						$\var{nb}.\var{parent} = \var{x} $\;
						move $\var{nb}$ right behind $\var{x}$ in $\var{nowlist}$ \;
					}
				}
				\uElseIf{$\var{nb}.\var{status}$ == $ later$}{
					calculate new distance estimation \var{dist} \;
					\If{$\var{dist} < \var{nb}.\var{distanceEstimation}$}{
						$\var{nb}.\var{distanceEstimation} = \var{dist}$ \;
						$\var{nb}.\var{parent} = \var{x} $\;
						remove $\var{nb}$ from $\var{laterlist}$ \;					
						insert $\var{nb}$ right behind $\var{x}$ in $\var{nowlist}$ \nllabel{code:moveback}\;
						$\var{nb}.\var{status} \gets \var{now}$ \;
					}
				}
				\ElseIf{$nb.status == inactive$}{
					calculate $\var{nb}.\var{distanceEstimation}$ \;
					$\var{nb}.\var{parent} ? \var{x}$\;
					calc $nb.g$, $nb.f$ and set $nb.parent \leftarrow x$\;
					insert $\var{nb}$ right behind $\var{x}$ in $\var{nowlist}$ \;
					$\var{nb}.\var{status} \gets \var{now} $ \;
				}
			}
			remove $\var{x}$ from $\var{nowlist}$ \;
		}{
			$\var{x}.\var{status} \gets \var{later}$\;
 			move $\var{x}$ into \var{laterlist} \nllabel{code:move}\;
 		}
	}
	increase $\var{threshold}$ \nllabel{code:thresh} \;
	swap $\var{nowlist} \leftrightarrow laterlist$ \;
	swap the values of $\var{now}$ and $\var{later}$ \nllabel{code:swap} \;
}
return -1 \tcp*[r]{no existing path to end}
\caption{parallelizable Fringe Search with two lists\label{algo:par}. Note: With the locking mechanism described in section \ref{ssec:lock} and in figures \ref{fig:lock}, \ref{fig:insert} and \ref{fig:remove}), this algorithm can be parallelized.}
\end{algorithm}

\mypar{Swapping the lists}
Whenever we relax the threshold the nowlist and the laterlist will get swapped (line \ref{code:swap} in algorithm \ref{algo:par}). This means the nowlist will become the laterlist and vice versa. Also the node states will get swapped (now will become later and vice versa).\\
This part has to be done sequentially. After this we can run in parallel again but we don't want to have all the threads starting from the first node again in the nowlist again but rather have them distributed over the whole nowlist. We achieve this by remembering the last node in the laterlist a thread has worked with (e.g. a node moved into the laterlist in line \ref{code:move} in algorithm \ref{algo:par}). Like this, the probability is high that this node is now in the nowlist and not all threads will start at the same node. Another advantage of this behaviour is that we might profit from the cache effect because this node might still be in the cache. But of course we have to check if this node is now really in the nowlist which can't be guaranteed (a node remembered in line \ref{code:move} could be moved back in line \ref{code:moveback}). If this is not the case we just start from the first node in the new nowlist.

\subsection{Locking}\label{ssec:lock}

\mypar{Traversing the list}
Whenever a thread traverses the nowlist it tries to lock every node it encounters. It does not force a lock because this would slow the application down significantly (many threads would wait at the same node)! It just tries and if it's successful it can work with this node (visit the adjacent nodes, etc.). If the thread fails trying to lock the node it means that another thread is working with exactly this node.\\
In this case we don't go just to the next node in the list because this might lead to some jam because the other thread will most likely try to lock this node soon and like this the two threads would get in each others way quite often.\\
So instead of just going to the next node if we encounter a locked node we skip a few nodes in order to achieve a nice distribution of the threads over the nowlist. Skipping 150 nodes has proven to provide a relatively good distribution which results in a faster runtime for large graphs (larger than $10^7$ nodes).

\mypar{Lock type}
We implemented and tried several locks. Next to the locks provided by OpenMP we implemented other locks using inline assembly (e.g. test-and-set lock). 

\mypar{Avoiding deadlocks}
In order to avoid deadlocks we always lock in the same direction (see figure \ref{fig:lock}). If we don't know whether the second node we would like to lock is in the correct direction (e.g. left in the nowlist) we must not force a lock. This can only occur if we want to move a node in the nowlist and this is only the case if we've found a shorter path to a node that already has a path that is below the previous threshold. In this case we can try locking it. If the try is successful we can update the path and move it and if not we just leave the old path that was already below the old threshold. This does not change the behaviour of the algorithm since the other node was also in the nowlist and so it could have been closed by another thread anyway. 
If we lock a node outside the two lists, this can be done without any problems, because we never try to lock more than one node outside the lists.
Acting like this, we prevent any possible circular wait dependencies and no deadlocks can occur \cite{Coffman:71}.

\begin{figure}[h]\centering
  \includegraphics[scale=0.38]{locking.eps}
  \caption{Locking direction \label{fig:lock}}
\end{figure}

\mypar{Insert nodes}
The sequence for inserting nodes is shown in figure \ref{fig:insert}.

\begin{figure}[h]\centering
  \includegraphics[scale=0.31]{insert.eps}
  \caption{Insert a node into the list \label{fig:insert}}
\end{figure}

\mypar{Remove nodes}
The sequence for removing nodes is shown in  figure \ref{fig:remove}.\\
We implemented and tested two different strategies for removing the nodes. The first strategy is to remove the node immediately as shown in figure \ref{fig:remove} when it has to be removed. The other strategy is to just mark it as removed, go on and let other threads actually remove it while they are traversing the list (lazy deletion).

\mypar{Acquiring locks}
All locks are acquired by \"optimistic locking\". That means normally we try to get locks on nodes with specific properties (e.g. the node must be predecessor of another, the node must have a specific state). Now we spin until we get the lock, afterwards we test again, if the conditions are met and if not we release the lock immediately.

\begin{figure}[h]\centering
  \includegraphics[scale=0.31]{remove.eps}
  \caption{Remove a node from the list. Note: The pointers of the removed node remain so that a traversing thread won't get lost (traversing is without locks). \label{fig:remove}}
\end{figure}



\section{Experimental Results}\label{sec:exp}

In this section we evaluate our implementation by looking at the speed of the sequential algorithm, the performance of the different locks, strong scaling, weak scaling, the speedup with different threshold relaxation values and the quality (length) of the path depending on the threshold relaxation value.

\subsection{Experimental setup}\label{ssec:setup}

\mypar{Hardware and compiler}
The experiments have been done on kanifushi.inf.ethz.ch, a computer with the following properties:
\begin{compactitem}
\item NUMA model with 32 CPUs on 4 nodes
\item 8 CPUs per node
\item Intel(R) Xeon(R) CPU E7- 4830 @ 2.13GHz
\item per CPU: 32KB L1 cache, 256KB L2 cache
\item per node: 24MB L3 cache, 16GB memory
\end{compactitem}
Because the it's an implementation for a shared memory environment we've used only 1 node with 8 CPUs for the experiments. In order to do this the application had to be executed with the \textit{numactl} tool (e.g. \textit{numactl --cpunodebind 0 --membind 0 fringe}).\\
The code has been compiled with g++ v. 4.6.1 using O1 optimization. All of the following performance analysis plots are the result of 50 runs on 1 node.

\mypar{Graphs used for benchmarking}
Each experiment has been run on two different graph types with different obstacles, the "cross graph" and the "circle graph" which both are illustrated in figure \ref{fig:graphs}. Both graphs have the following properties:
\begin{compactitem}
\item based on a regular grid with distance 1 and 8 edges per node
\item each node is moved randomly\\(normal distribution, $\sigma = 0.3$)
\item start node is top left and end node is bottom right
\end{compactitem}

\begin{figure}[h]\centering
  \includegraphics[scale=0.3]{benchmark_graphs.eps}
  \caption{The graphs used for benchmarking: "cross graph" (left) and "circle graph" (right). In this picture the blue nodes represent the found path, the green nodes are the closed nodes and the red nodes were in the nowlist or laterlist when the algorithm finished. \label{fig:graphs}}
\end{figure}

\subsection{Results}\label{ssec:results}

\mypar{Sequential Fringe Search}
In order to affirm good performance of the sequential version we tested the sequential Fringe search against the A* search from the Boost Graph Library\footnote{\url{http://www.boost.org/doc/libs/1_55_0/libs/graph/doc/index.html}} as we couldn't find a reliable implementation of Fringe Search we could compare our code to.\\
Our implementation of Fringe Search proved to be much faster than A* from the Boost Graph Library (about 10 times as fast on a graph with $1024 \times 1024$ nodes). But of course one has to take into account that Fringe Search doesn't guarantee an optimal path like A*.

\mypar{Locks}
As mentioned in section \ref{ssec:lock}, next to the locks provided by OpenMP, we implemented some other locks using inline assembly. We implemented the following additional locks:
\begin{compactitem}
\item TAS: test-and-set lock
\item TAS EXP: test-and-set lock with exponential back-off
\end{compactitem}
As you can see in figure \ref{fig:lock_bench} the inline assembly locks were significantly faster than the OpenMP locks. Therefore we used the test-and-set lock with exponential back-off for our implementation and also the following benchmarks are based on the implementation with these locks.

\begin{figure}[h]\centering
  \includegraphics[scale=0.558]{lock_benchmark.eps}
  \caption{Strong scaling using different locks.\label{fig:lock_bench}}
\end{figure}


\mypar{Strong scaling}
One of the most important questions is how much faster the application gets by using more cores for a fixed problem size. The results are shown in figure \ref{fig:strong_scaling}.
\begin{figure}[h]\centering
  \includegraphics[scale=0.558]{strong_scaling.eps}
  \caption{Strong scaling \label{fig:strong_scaling}}
\end{figure}
For both benchmark graphs the parallel version is faster than the sequential version if it uses 2 or more cores. Using only 1 core it's naturally slower because of the additional overhead with the locking.\\
One can also see that the more cores we use, the lower the actual benefit in terms of speedup gets. This is consistent with Amdahl's law as the sequential part of the program remains the same.\\
Also, it actually doesn't matter which strategy for removing the nodes we use (lazy vs. normal - see section \ref{ssec:lock}). The difference in speed is negligible.

\mypar{Weak scaling}
Because of the limitations in terms of speedup according to Amdahl's law which are also visible in figure \ref{fig:strong_scaling} the next important question is how the runtime behaves with an increasing problem size for an increasing number of cores. This means we keep the problem size per core (number of nodes per core) constant.\\
As you can see in figure \ref{fig:weak_scaling} we still experience an increase in runtime with increasing problem size, but it's not that much and certainly not as dramatic as if it would be if we didn't increase the number of cores.
\begin{figure}[h]\centering
  \includegraphics[scale=0.558]{weak_scaling.eps}
  \caption{Weak scaling\label{fig:weak_scaling}}
\end{figure}
The behaviour meets our expectations, because with increasing problem size we also increase the time the program spends in the sequential part (longer path will lead to more threshold updates).


\mypar{Path length versus $\#$ cores}
One interesting question was whether the path length gets worse the more threads we have. As mentioned in section \ref{ssec:lock} (avoiding deadlocks) we cannot always update a node with a better path if it's already below threshold and locked by another thread. And the more threads we have the higher is the possibility that this happens. This behaviour is not ideal but it's still consistent with the definition of the algorithm.
\begin{figure}[h]\centering
  \includegraphics[scale=0.558]{error_cores.eps}
  \caption{Relative error in path length compared to A* depending on the number of cores\label{fig:error_cores}}
\end{figure}
As shown in figure \ref{fig:error_cores} the number of cores/threads does not affect the quality of the path.


\mypar{Path length versus threshold relaxation}
The threshold relaxation value defines by how much we increase the threshold once we don't have any more nodes below threshold left. If we increase the threshold only by a small value we will just consider few new nodes but they all are relatively promising nodes and result in a short path. If we increase the threshold by a bigger value we'll have more nodes to look at, but not all of them will be that good in terms of path length.
\begin{figure}[h]\centering
  \includegraphics[scale=0.558]{error_threshold.eps}
  \caption{Relative error in path length compared to shortest path (A*) depending on threshold relaxation value\label{fig:error_thresh}}
\end{figure}
As you can see in figure \ref{fig:error_thresh} the relative error gets quite high once we have a threshold relaxation value greater than 1 whereas the path quality is very good for small threshold relaxation values.

\mypar{Runtime versus threshold relaxation}
As shown in figure \ref{fig:error_thresh} we can have a very good path with a low threshold relaxation value. However, by doing this we always just have a few nodes in our nowlist and we have to increase the threshold more often, which is not good because it's the sequential part of the algorithm. Therefore we expect our algorithm to be slower the smaller the threshold relaxation value is, which is also the case as can be seen in figure \ref{fig:runtime_thresh}.
\begin{figure}[h]\centering
  \includegraphics[scale=0.558]{runtime_threshold.eps}
  \caption{Runtime depending on threshold relaxation value\label{fig:runtime_thresh}}
\end{figure}
One can also see that the runtime for the sequential version gets slower if the relaxation value is too high. With a high value the algorithm will visit many nodes, even the not so promising ones. So we will have quite a big fan-out (compare with figure \ref{fig:graphs}) that one core alone won't be able to handle in sufficient time. By running the application in parallel there are more cores that can handle these relatively big nowlist and as shown in figure \ref{fig:speedup_thresh}, the higher the threshold relaxation value is the more speedup we get with the parallel version and of course, more cores will result in a higher speedup.
\begin{figure}[h]\centering
  \includegraphics[scale=0.558]{speedup_threshold.eps}
  \caption{Speedup of parallel version versus sequential version using different threshold relaxation values\label{fig:speedup_thresh}}
\end{figure}


\section{Conclusions}

We implemented a sequential version of Fringe Search that was significantly faster than the A* implementation from the Boost Graph Library. We implemented it as a parallel version for a shared memory environment. The important results are:
\begin{itemize}
\item The parallel version running on only two cores was already faster than the sequential version.
\item The parallel version didn't have any noticeable compromises in terms of path quality.
\item Strong scaling has shown to be quite good, but of course limited by having a sequential part in the algorithm.
\item Weak scaling is good but not perfect due to the increasing sequential part with increasing graph size.
\item The implementation can easily be tuned with the threshold relaxation parameter in order to meet the individual requirements concerning path quality and runtime.
\end{itemize}
We haven't found a solution to get rid of the sequential part (threshold relaxation and list swapping) and we are not sure if there exists one, but nevertheless we believe that this is a relatively fast implementation on a shared memory environment.\\
Possible next steps could be enhancing the application to make it also work in a distributed memory environment or in a mix of both.



% References should be produced using the bibtex program from suitable
% BiBTeX files (here: bibl_conf). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{bibl_conf}

\end{document}

